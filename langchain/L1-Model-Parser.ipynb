{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1b1bbc3",
      "metadata": {},
      "source": [
        "# LangChain Course One"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead3cd66",
      "metadata": {},
      "source": [
        "![alt text](./images/image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97b6fe55",
      "metadata": {},
      "source": [
        "## Video & Github\n",
        "[LangChain-Tutior](https://github.com/ConnectAI-E/LangChain-Tutior)\n",
        "\n",
        "[Bilibli Video](https://www.bilibili.com/video/BV1jh4y1X7cq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e0202e3",
      "metadata": {},
      "source": [
        "solved in README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c2f0cd67",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -Uq langchain langchain-core langchain-community langchain-ollama langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687a6f95",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "langchain                 0.3.27\n",
            "langchain-community       0.3.27\n",
            "langchain-core            0.3.74\n",
            "langchain-ollama          0.3.7\n",
            "langchain-text-splitters  0.3.9\n"
          ]
        }
      ],
      "source": [
        "%pip list | findstr \"langchain langchain-core langchain-community langchain-ollama\"\n",
        "# langchain                 0.3.27\n",
        "# langchain-community       0.3.27\n",
        "# langchain-core            0.3.74\n",
        "# langchain-ollama          0.3.7\n",
        "# langchain-text-splitters  0.3.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1e869f6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q langchain-core==0.3.74 langchain-community==0.3.27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "31110231",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "langchain-core            0.3.27\n"
          ]
        }
      ],
      "source": [
        "%pip list | findstr \"langchain-core\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dfe747ef",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-20T16:14:29.364982Z",
          "start_time": "2025-08-20T16:14:28.450588Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "global.index-url='http://mirrors.aliyun.com/pypi/simple/'\n",
            "install.trusted-host='mirrors.aliyun.com'\n"
          ]
        }
      ],
      "source": [
        "%pip config list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d490be2",
      "metadata": {},
      "source": [
        "## OllamaLLM Simple invoke\n",
        "\n",
        "invoke with string message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "59facc4c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-20T16:19:07.296146Z",
          "start_time": "2025-08-20T16:19:07.251537Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "\n",
            "# This is a simple Python program that prints \"Hello, World!\" to the console.\n",
            "\n",
            "print(\"Hello, World!\")\n",
            "\n",
            "```\n",
            "\n",
            "To run this code, save it in a file named `hello_world.py`, and execute it using a Python interpreter or command prompt with `python hello_world.py`. The expected output will be:\n",
            "\n",
            "```\n",
            "\n",
            "Hello, World!\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "llm = OllamaLLM(model=\"phi3:mini\")\n",
        "\n",
        "result = llm.invoke(\"写一个Hello World的Python程序\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1be45a8",
      "metadata": {},
      "source": [
        "invoke with string HumanMessage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "eb8ba200",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "在Python中，列表（List）与元组（Tuple）都是不可变序列类型。然而，他们之间存在几个重要的差异：\n",
            "\n",
            "1. 拆分: Python中，列表可以通过解厉解或倒序方式进行拆分，不依赖于索引。相比之下，元组是不能拆开的。例如：\n",
            "   ```python\n",
            "   my_list = [1, 2, 'A', (3,4)] # List can contain mixed data types including other lists and tuples.\n",
            "   \n",
            "   a, b = my_list[0], mythy_list[1] # This will raise an error because you cannot unpack into two variables if the length of elements doesn't match. Use: for item in my_list to iterate over it without indexing (more Pythonic).\n",
            "   \n",
            "   x, y, z = 1, 'B', (5,6) # Unpacks correctly as there are matching count and tuple is unchangeable structure inside list which makes this a reliable option.\n",
            "   ```\n",
            "\n",
            "2. 修改: 列表可以修改内容，但元组不允许。比如：\n",
            "   ```python\n",
            "    my_list = [1, 'B', (5,6)] # List can contain a tuple as an element and it's changeable. But Tuples are immutable. They cannot be changed once they have been created. This makes them faster to process than lists but not very flexible for data manipulation which is the main advantage of list over tuples in Python programming language.\n",
            "    \n",
            "    my_list[1] = 'C' # this will change element at index 1 from 'B' to 'C'. But attempting something like: `my_tuple`[0]='E', would raise a TypeError exception, because you cannot modify the tuple once it is created (it’s immutable).\n",
            "   ```\n",
            "\n",
            "3. Memory Usage and Speed: Tuples are stored in memory more efficiently than lists due to their immutability which makes them faster for processing data that won't change throughout your program execution. Therefore, if you know a collection of items will not be modified during the lifetime of its variables, it is recommended to use tuple instead of list as they can save both time and computer resources while working with Python lists tend to consume more memory due to their flexibility which makes them slower in processing tasks compared tuples when data size increases. This leads into an important discussion about trade-offs between readability/flexibility (lists) vs performance & safety from unintentional changes(tuples).\n",
            "   \n",
            "4. Functionality: Python provides built-in methods for lists, such as append(), extend() and remove(). However, there are no native functions in tuples that alter the content of its elements since they're immutable by design which means you cannot change their values once set unlike Lists. For example list can do `my_list.append('D')`, but tuple has nothing similar for this functionality hence it needs to be changed into a mutable object (like list or dictionary) before any such operation is performed upon its content inside the Tuple:\n",
            "   ```python\n",
            "    my_tuple = ('A', 'B', 123, [456]) # A List can be nested in tuple. But you cannot change contents of inner-list directly from Tuple due to immutability which leads into another advantage where data security is improved by using tuples when working with sensitive information since unauthorized changes could potentially harm the integrity of your application's state and behavior as a result, making it crucial for programmers understanding this difference between List & Tuple in Python.\n",
            "    ```\n",
            "5. Syntax: The syntax to declare tuple is `(value1, value2)`, which can be empty or contain more than two values separated by commas while list uses `[]` with similar structure but has the added capability of storing other lists and tuples inside it making them very flexible in nature for handling complex nested data structures.\n",
            "    ```python\n",
            "       my_list = [1, 2] # Lists can contain various types (integers, strings etc.) as well as other containers like list or tuple: `my_list[0]` would return integer '1' while `my_list[1]` returns string \"B\". Tuples are declared in a similar manner but they cannot have nested lists or tuples inside.\n",
            "    ```\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "chat = ChatOllama(model=\"phi3:mini\")\n",
        "\n",
        "response = chat.invoke([HumanMessage(content=\"解释一下Python中列表和元组的区别\")])\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "392affde",
      "metadata": {},
      "source": [
        "## Chain Stream Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "727470fd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-20T16:11:13.816667Z",
          "start_time": "2025-08-20T16:11:13.238642Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "在Python中，装饰器是一个特殊的函数或类，它可以修改现有函数或方法的行为，而不修改原始代码。装饰器通常用于多重绑定、缓存、日志记录、限制、并发控制等场景。\n",
            "\n",
            "Python提供了`@decorator`语法来表示一个装饰器。下面是一个基本的装饰器的例子：\n",
            "\n",
            "```python\n",
            "def my_decorator(func):\n",
            "    def wrapper():\n",
            "        print(\"Before function execution.\")\n",
            "        func()\n",
            "        print(\"After function execution.\")\n",
            "        \n",
            "    return wrapper\n",
            "\n",
            "@my_decorator\n",
            "def say_hello():\n",
            "    print(\"Hello!\")\n",
            "\n",
            "say_hello()  # Output: Before function execution. Hello! After function execution.\n",
            "```\n",
            "\n",
            "在上面的例子中，`my_decorator`函数被用作一个装饰器来装饰`say_hello`函数。"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"你是一个Python助手。\n",
        "请根据下面的问题给出详细解答：\n",
        "{question}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "llm = OllamaLLM(\n",
        "    model=\"phi3:mini\",\n",
        "    num_predict=256, # limit length\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "for chunk in chain.stream({\"question\": \"解释一下Python的装饰器\"}):\n",
        "    print(chunk, end=\"\", flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1f885a1",
      "metadata": {},
      "source": [
        "## Prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e068f98d",
      "metadata": {},
      "outputs": [],
      "source": [
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3e98c989",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9b94992b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ab9dcfc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_style = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\"\n",
        "\n",
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "47473c46",
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7356556b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n"
          ]
        }
      ],
      "source": [
        "print(type(customer_messages))\n",
        "print(type(customer_messages[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8d68eb00",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "chat = ChatOllama(model=\"phi3:mini\")\n",
        "response = chat.invoke(customer_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7beca843",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"Arrr, this morning was quite upsetting for me when my blender lid unexpectedly came off and splattered smoothie all over my kitchen walls. To make matters more frustrating, the warranty doesn't cover cleaning expenses.\" I seek assistance right away!\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5cef55fe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "service_reply = \"\"\"Hey there customer, \\\n",
        "the warranty does not cover \\\n",
        "cleaning expenses for your kitchen \\\n",
        "because it's your fault that \\\n",
        "you misused your blender \\\n",
        "by forgetting to put the lid on before \\\n",
        "starting the blender. \\\n",
        "Tough luck! See ya!\n",
        "\"\"\"\n",
        "\n",
        "service_style_pirate = \"\"\"\\\n",
        "a polite tone \\\n",
        "that speaks in English Pirate\\\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "service_messages = prompt_template.format_messages(\n",
        "    style=service_style_pirate,\n",
        "    text=service_reply)\n",
        "\n",
        "print(service_messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "68f184d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "service_response = ChatOllama(model=\"phi3:mini\").invoke(service_messages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2f2f7f15",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ahoy matey! Here be yer shipmate with a bit o' news from me end: ```Hey there customer, regretfully, we cannot extend our warranty coverage for cleanin' costs of your kitchen appliances. 'Tis due to thar misuse by forgettin' t'secure the lid before givin' yer blender a go. Bummer! Fare thee well on urgent errands until next time we speak, savvy?\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(service_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d841f2",
      "metadata": {},
      "source": [
        "## Output Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3c9854f2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# wanna the output below\n",
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1391e18c",
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "645de4f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n",
            "[HumanMessage(content=\"For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\n\\n\", additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "print(prompt_template)\n",
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "print(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c9e7ac91",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"gift\": true,\n",
            "    \"delivery_days\": 2,\n",
            "    \"price_value\": [\n",
            "        \"It's slightly more expensive than the other leaf blowers out there\",\n",
            "        \"I think it's worth it for the extra features\"\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "review_response = ChatOllama(model=\"phi3:3.8b\").invoke(messages)\n",
        "\n",
        "print(review_response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a015807c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7201bf6a",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'get'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# You will get an error by running this line of code \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# because 'gift' is not a dictionary\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 'gift' is a string\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mgift\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'get'"
          ]
        }
      ],
      "source": [
        "# You will get an error by running this line of code \n",
        "# because 'gift' is not a dictionary\n",
        "# 'gift' is a string\n",
        "response.content.get('gift')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "570a0fab",
      "metadata": {},
      "source": [
        "## Parse the LLM output string into a Python dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "640d19df",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "gift_schema = ResponseSchema(name=\"gift\",\n",
        "                             description=\"Was the item purchased\\\n",
        "                             as a gift for someone else? \\\n",
        "                             Answer True if yes,\\\n",
        "                             False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
        "                                      description=\"How many days\\\n",
        "                                      did it take for the product\\\n",
        "                                      to arrive? \\\n",
        "                                      If this information is not found,\\\n",
        "                                      output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\",\n",
        "                                    description=\"Extract any\\\n",
        "                                    sentences about the value or \\\n",
        "                                    price, and output them as a \\\n",
        "                                    comma separated Python list.\")\n",
        "\n",
        "response_schemas = [gift_schema, \n",
        "                    delivery_days_schema,\n",
        "                    price_value_schema]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "babe3ccc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d34ec860",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? Type is numeric, not wrapped by quote.                                       If this information is not found,                                      output -1.\n",
            "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d627277",
      "metadata": {},
      "outputs": [],
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product\\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "messages = prompt.format_messages(text=customer_review, \n",
        "                                format_instructions=format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3c754b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? Type is numeric, not wrapped by quote.                                       If this information is not found,                                      output -1.\n",
            "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f3cebe4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "\t\"gift\": \"True\",\n",
            "\t\"delivery_days\": \"2\",\n",
            "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "review_response1 = ChatOllama(model=\"phi3:3.8b\").invoke(messages)\n",
        "\n",
        "print(review_response1.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3af301",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'gift': 'True', 'delivery_days': '2', 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_dict = output_parser.parse(review_response1.content)\n",
        "print(output_dict)\n",
        "type(output_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82cb5f1c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_dict.get('delivery_days')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11969950",
      "metadata": {},
      "source": [
        "## nomic-embed-small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e53d4ef7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "嵌入向量维度: 768\n",
            "前10个向量值: [0.040841665, 0.06760364, 0.0076618423, 0.00079046516, -0.023915213, 0.018770786, 0.0035362986, 0.0054711085, 0.062290493, 0.075273275]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "# 输入文本\n",
        "text = \"你好，今天的天气怎么样？\"\n",
        "\n",
        "# 调用 Ollama API\n",
        "resp = requests.post(\n",
        "    \"http://localhost:11434/api/embed\",\n",
        "    json = {\n",
        "        \"model\": \"nomic-embed-small\",  # 你导入时的模型名\n",
        "        \"input\": text\n",
        "    }\n",
        ")\n",
        "\n",
        "data = resp.json()\n",
        "print(\"嵌入向量维度:\", len(data[\"embeddings\"][0]))\n",
        "print(\"前10个向量值:\", data[\"embeddings\"][0][:10])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
