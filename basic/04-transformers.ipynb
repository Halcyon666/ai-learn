{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c4b537",
   "metadata": {},
   "source": [
    "# transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7bbc46f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2769, 6230, 2533,  679, 6121,  102,    0,    0,    0],\n",
      "        [ 101, 2769, 6230, 2533, 1071, 2141, 6820, 1377,  809,  102]]) \n",
      " <class 'torch.Tensor'>\n",
      "torch.Size([2, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769, 6230, 2533,  679, 6121,  102,    0,    0,    0],\n",
       "        [ 101, 2769, 6230, 2533, 1071, 2141, 6820, 1377,  809,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese', use_fast=True)\n",
    "\n",
    "sentences = [\"我觉得不行\", \"我觉得其实还可以\"]\n",
    "\n",
    "batch_inputs = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(batch_inputs[\"input_ids\"], \"\\n\", type(batch_inputs[\"input_ids\"]))\n",
    "print(batch_inputs[\"input_ids\"].shape)\n",
    "\n",
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76abb258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]) \n",
      " <class 'tokenizers.Encoding'>\n"
     ]
    }
   ],
   "source": [
    "# slow tokenizer才有 Encoding\n",
    "print(batch_inputs[0], \"\\n\", type(batch_inputs[0]))\n",
    "\n",
    "# torch.Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5086060",
   "metadata": {},
   "source": [
    "| 写法                          | 实际含义                           |\n",
    "| --------------------------- | ------------------------------ |\n",
    "| `batch_inputs[\"input_ids\"]` | 从 `data` 里取模型输入                |\n",
    "| `batch_inputs[i]`           | 从 `encodings` 里取第 i 个 Encoding |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f427cb5",
   "metadata": {},
   "source": [
    "## Model Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d98303e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0096b2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出对象的类型: <class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n",
      "向量形状: torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    # **batch_inputs 字典解包\n",
    "    outputs = model(**batch_inputs)\n",
    "\n",
    "print(f\"输出对象的类型: {type(outputs)}\")\n",
    "# 核心产出：last_hidden_state (最后一层的隐藏状态)\n",
    "print(f\"向量形状: {outputs.last_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1923f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 提示：取出第 0 个位置的 token 向量\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :] \n",
    "print(cls_embeddings.shape) # 应该是 [2, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "580a115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight True\n",
      "embeddings.position_embeddings.weight True\n",
      "embeddings.token_type_embeddings.weight True\n",
      "embeddings.LayerNorm.weight True\n",
      "embeddings.LayerNorm.bias True\n",
      "encoder.layer.0.attention.self.query.weight True\n",
      "encoder.layer.0.attention.self.query.bias True\n",
      "encoder.layer.0.attention.self.key.weight True\n",
      "encoder.layer.0.attention.self.key.bias True\n",
      "encoder.layer.0.attention.self.value.weight True\n",
      "encoder.layer.0.attention.self.value.bias True\n",
      "encoder.layer.0.attention.output.dense.weight True\n",
      "encoder.layer.0.attention.output.dense.bias True\n",
      "encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "encoder.layer.0.intermediate.dense.weight True\n",
      "encoder.layer.0.intermediate.dense.bias True\n",
      "encoder.layer.0.output.dense.weight True\n",
      "encoder.layer.0.output.dense.bias True\n",
      "encoder.layer.0.output.LayerNorm.weight True\n",
      "encoder.layer.0.output.LayerNorm.bias True\n",
      "encoder.layer.1.attention.self.query.weight True\n",
      "encoder.layer.1.attention.self.query.bias True\n",
      "encoder.layer.1.attention.self.key.weight True\n",
      "encoder.layer.1.attention.self.key.bias True\n",
      "encoder.layer.1.attention.self.value.weight True\n",
      "encoder.layer.1.attention.self.value.bias True\n",
      "encoder.layer.1.attention.output.dense.weight True\n",
      "encoder.layer.1.attention.output.dense.bias True\n",
      "encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "encoder.layer.1.intermediate.dense.weight True\n",
      "encoder.layer.1.intermediate.dense.bias True\n",
      "encoder.layer.1.output.dense.weight True\n",
      "encoder.layer.1.output.dense.bias True\n",
      "encoder.layer.1.output.LayerNorm.weight True\n",
      "encoder.layer.1.output.LayerNorm.bias True\n",
      "encoder.layer.2.attention.self.query.weight True\n",
      "encoder.layer.2.attention.self.query.bias True\n",
      "encoder.layer.2.attention.self.key.weight True\n",
      "encoder.layer.2.attention.self.key.bias True\n",
      "encoder.layer.2.attention.self.value.weight True\n",
      "encoder.layer.2.attention.self.value.bias True\n",
      "encoder.layer.2.attention.output.dense.weight True\n",
      "encoder.layer.2.attention.output.dense.bias True\n",
      "encoder.layer.2.attention.output.LayerNorm.weight True\n",
      "encoder.layer.2.attention.output.LayerNorm.bias True\n",
      "encoder.layer.2.intermediate.dense.weight True\n",
      "encoder.layer.2.intermediate.dense.bias True\n",
      "encoder.layer.2.output.dense.weight True\n",
      "encoder.layer.2.output.dense.bias True\n",
      "encoder.layer.2.output.LayerNorm.weight True\n",
      "encoder.layer.2.output.LayerNorm.bias True\n",
      "encoder.layer.3.attention.self.query.weight True\n",
      "encoder.layer.3.attention.self.query.bias True\n",
      "encoder.layer.3.attention.self.key.weight True\n",
      "encoder.layer.3.attention.self.key.bias True\n",
      "encoder.layer.3.attention.self.value.weight True\n",
      "encoder.layer.3.attention.self.value.bias True\n",
      "encoder.layer.3.attention.output.dense.weight True\n",
      "encoder.layer.3.attention.output.dense.bias True\n",
      "encoder.layer.3.attention.output.LayerNorm.weight True\n",
      "encoder.layer.3.attention.output.LayerNorm.bias True\n",
      "encoder.layer.3.intermediate.dense.weight True\n",
      "encoder.layer.3.intermediate.dense.bias True\n",
      "encoder.layer.3.output.dense.weight True\n",
      "encoder.layer.3.output.dense.bias True\n",
      "encoder.layer.3.output.LayerNorm.weight True\n",
      "encoder.layer.3.output.LayerNorm.bias True\n",
      "encoder.layer.4.attention.self.query.weight True\n",
      "encoder.layer.4.attention.self.query.bias True\n",
      "encoder.layer.4.attention.self.key.weight True\n",
      "encoder.layer.4.attention.self.key.bias True\n",
      "encoder.layer.4.attention.self.value.weight True\n",
      "encoder.layer.4.attention.self.value.bias True\n",
      "encoder.layer.4.attention.output.dense.weight True\n",
      "encoder.layer.4.attention.output.dense.bias True\n",
      "encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "encoder.layer.4.intermediate.dense.weight True\n",
      "encoder.layer.4.intermediate.dense.bias True\n",
      "encoder.layer.4.output.dense.weight True\n",
      "encoder.layer.4.output.dense.bias True\n",
      "encoder.layer.4.output.LayerNorm.weight True\n",
      "encoder.layer.4.output.LayerNorm.bias True\n",
      "encoder.layer.5.attention.self.query.weight True\n",
      "encoder.layer.5.attention.self.query.bias True\n",
      "encoder.layer.5.attention.self.key.weight True\n",
      "encoder.layer.5.attention.self.key.bias True\n",
      "encoder.layer.5.attention.self.value.weight True\n",
      "encoder.layer.5.attention.self.value.bias True\n",
      "encoder.layer.5.attention.output.dense.weight True\n",
      "encoder.layer.5.attention.output.dense.bias True\n",
      "encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "encoder.layer.5.intermediate.dense.weight True\n",
      "encoder.layer.5.intermediate.dense.bias True\n",
      "encoder.layer.5.output.dense.weight True\n",
      "encoder.layer.5.output.dense.bias True\n",
      "encoder.layer.5.output.LayerNorm.weight True\n",
      "encoder.layer.5.output.LayerNorm.bias True\n",
      "encoder.layer.6.attention.self.query.weight True\n",
      "encoder.layer.6.attention.self.query.bias True\n",
      "encoder.layer.6.attention.self.key.weight True\n",
      "encoder.layer.6.attention.self.key.bias True\n",
      "encoder.layer.6.attention.self.value.weight True\n",
      "encoder.layer.6.attention.self.value.bias True\n",
      "encoder.layer.6.attention.output.dense.weight True\n",
      "encoder.layer.6.attention.output.dense.bias True\n",
      "encoder.layer.6.attention.output.LayerNorm.weight True\n",
      "encoder.layer.6.attention.output.LayerNorm.bias True\n",
      "encoder.layer.6.intermediate.dense.weight True\n",
      "encoder.layer.6.intermediate.dense.bias True\n",
      "encoder.layer.6.output.dense.weight True\n",
      "encoder.layer.6.output.dense.bias True\n",
      "encoder.layer.6.output.LayerNorm.weight True\n",
      "encoder.layer.6.output.LayerNorm.bias True\n",
      "encoder.layer.7.attention.self.query.weight True\n",
      "encoder.layer.7.attention.self.query.bias True\n",
      "encoder.layer.7.attention.self.key.weight True\n",
      "encoder.layer.7.attention.self.key.bias True\n",
      "encoder.layer.7.attention.self.value.weight True\n",
      "encoder.layer.7.attention.self.value.bias True\n",
      "encoder.layer.7.attention.output.dense.weight True\n",
      "encoder.layer.7.attention.output.dense.bias True\n",
      "encoder.layer.7.attention.output.LayerNorm.weight True\n",
      "encoder.layer.7.attention.output.LayerNorm.bias True\n",
      "encoder.layer.7.intermediate.dense.weight True\n",
      "encoder.layer.7.intermediate.dense.bias True\n",
      "encoder.layer.7.output.dense.weight True\n",
      "encoder.layer.7.output.dense.bias True\n",
      "encoder.layer.7.output.LayerNorm.weight True\n",
      "encoder.layer.7.output.LayerNorm.bias True\n",
      "encoder.layer.8.attention.self.query.weight True\n",
      "encoder.layer.8.attention.self.query.bias True\n",
      "encoder.layer.8.attention.self.key.weight True\n",
      "encoder.layer.8.attention.self.key.bias True\n",
      "encoder.layer.8.attention.self.value.weight True\n",
      "encoder.layer.8.attention.self.value.bias True\n",
      "encoder.layer.8.attention.output.dense.weight True\n",
      "encoder.layer.8.attention.output.dense.bias True\n",
      "encoder.layer.8.attention.output.LayerNorm.weight True\n",
      "encoder.layer.8.attention.output.LayerNorm.bias True\n",
      "encoder.layer.8.intermediate.dense.weight True\n",
      "encoder.layer.8.intermediate.dense.bias True\n",
      "encoder.layer.8.output.dense.weight True\n",
      "encoder.layer.8.output.dense.bias True\n",
      "encoder.layer.8.output.LayerNorm.weight True\n",
      "encoder.layer.8.output.LayerNorm.bias True\n",
      "encoder.layer.9.attention.self.query.weight True\n",
      "encoder.layer.9.attention.self.query.bias True\n",
      "encoder.layer.9.attention.self.key.weight True\n",
      "encoder.layer.9.attention.self.key.bias True\n",
      "encoder.layer.9.attention.self.value.weight True\n",
      "encoder.layer.9.attention.self.value.bias True\n",
      "encoder.layer.9.attention.output.dense.weight True\n",
      "encoder.layer.9.attention.output.dense.bias True\n",
      "encoder.layer.9.attention.output.LayerNorm.weight True\n",
      "encoder.layer.9.attention.output.LayerNorm.bias True\n",
      "encoder.layer.9.intermediate.dense.weight True\n",
      "encoder.layer.9.intermediate.dense.bias True\n",
      "encoder.layer.9.output.dense.weight True\n",
      "encoder.layer.9.output.dense.bias True\n",
      "encoder.layer.9.output.LayerNorm.weight True\n",
      "encoder.layer.9.output.LayerNorm.bias True\n",
      "encoder.layer.10.attention.self.query.weight True\n",
      "encoder.layer.10.attention.self.query.bias True\n",
      "encoder.layer.10.attention.self.key.weight True\n",
      "encoder.layer.10.attention.self.key.bias True\n",
      "encoder.layer.10.attention.self.value.weight True\n",
      "encoder.layer.10.attention.self.value.bias True\n",
      "encoder.layer.10.attention.output.dense.weight True\n",
      "encoder.layer.10.attention.output.dense.bias True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "encoder.layer.10.intermediate.dense.weight True\n",
      "encoder.layer.10.intermediate.dense.bias True\n",
      "encoder.layer.10.output.dense.weight True\n",
      "encoder.layer.10.output.dense.bias True\n",
      "encoder.layer.10.output.LayerNorm.weight True\n",
      "encoder.layer.10.output.LayerNorm.bias True\n",
      "encoder.layer.11.attention.self.query.weight True\n",
      "encoder.layer.11.attention.self.query.bias True\n",
      "encoder.layer.11.attention.self.key.weight True\n",
      "encoder.layer.11.attention.self.key.bias True\n",
      "encoder.layer.11.attention.self.value.weight True\n",
      "encoder.layer.11.attention.self.value.bias True\n",
      "encoder.layer.11.attention.output.dense.weight True\n",
      "encoder.layer.11.attention.output.dense.bias True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "encoder.layer.11.intermediate.dense.weight True\n",
      "encoder.layer.11.intermediate.dense.bias True\n",
      "encoder.layer.11.output.dense.weight True\n",
      "encoder.layer.11.output.dense.bias True\n",
      "encoder.layer.11.output.LayerNorm.weight True\n",
      "encoder.layer.11.output.LayerNorm.bias True\n",
      "pooler.dense.weight True\n",
      "pooler.dense.bias True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # if name == \"requires_grad\":\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e41b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": torch.tensor([[101, 2769, 6230, 2533, 679, 6121, 102]]),\n",
    "    \"attention_mask\": torch.tensor([[1, 1, 1, 1, 1, 1, 1]])\n",
    "}\n",
    "\n",
    "outputs = model(**inputs)\n",
    "loss = outputs.last_hidden_state.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(model.embeddings.word_embeddings.weight.grad is None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3b4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
